Â¡Excelente! ğŸ‘ Como vemos, `StandardScaler` transforma el conjunto de datos para que sus variables se aproximen a una distribuciÃ³n normal, de forma que todos datos tenga una [desviaciÃ³n estÃ¡ndar](https://es.wikipedia.org/wiki/Desviaci%C3%B3n_t%C3%ADpica) del mismo orden. De este modo, cada dato nos darÃ¡ una idea de a cuÃ¡ntos desvÃ­os de la media estÃ¡ ese punto.

Este tipo de procedimientos son muy importantes al realizar cualquier tipo de anÃ¡lisis de datos, y no sÃ³lo al trabajar con distancias euclÃ­deas. Imaginemos que tenemos que analizar la trayectoria profesional de dos personas, para hacer una selecciÃ³n laboral. :page_with_curl:  A priori, serÃ­a lÃ³gico pensar en basar esta selecciÃ³n en el currÃ­culum de dichas personas, Â¿no?

Sin embargo, el currÃ­culum no nos da un panorama completo de las habilidades de una persona. Por ejemplo, no nos permite conocer su capacidad de trabajo en equipo o sus habilidades para realizar mÃ¡s de una tarea a la vez, etc. Â¿QuÃ© peso le estamos dando entonces a estas otras caracterÃ­sticas? Â¿Estamos subvalorando o sobrevalorando las habilidades de las personas? Â¿QuÃ© pasa con las personas no tienen las mismas posibilidades para completar su curriculum? Â¿Las hace menos capaces para el trabajo?

En otras palabras, como ilustra la [esta historia breve](https://cajondeherramientas.com.ar/index.php/2016/05/05/en-bandeja-de-plata-una-historia-sobre-los-privilegios/) de [Toby Morris](https://en.wikipedia.org/wiki/Toby_Morris_(cartoonist)) es que resulta necesario escalar los datos para que todas las caracterÃ­sticas tengan la misma importancia y ninguna estÃ© dominada por otra.
