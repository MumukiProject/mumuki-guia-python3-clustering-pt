¡Excelente! 👏 Como vemos, `StandardScaler` transforma el conjunto de datos para que sus variables se aproximen a una distribución normal, de forma que todos datos tenga una [desviación estándar](https://es.wikipedia.org/wiki/Desviaci%C3%B3n_t%C3%ADpica) del mismo orden. De este modo, cada dato nos dará una idea de a cuántos desvíos de la media está ese punto.

Este tipo de procedimientos son muy importantes al realizar cualquier tipo de análisis de datos, y no sólo al trabajar con distancias euclídeas. Imaginemos que tenemos que analizar la trayectoria profesional de dos personas, para hacer una selección laboral. :page_with_curl:  A priori, sería lógico pensar en basar esta selección en el currículum de dichas personas, ¿no?

Sin embargo, el currículum no nos da un panorama completo de las habilidades de una persona. Por ejemplo, no nos permite conocer su capacidad de trabajo en equipo o sus habilidades para realizar más de una tarea a la vez, etc. ¿Qué peso le estamos dando entonces a estas otras características? ¿Estamos subvalorando o sobrevalorando las habilidades de las personas? ¿Qué pasa con las personas no tienen las mismas posibilidades para completar su curriculum? ¿Las hace menos capaces para el trabajo?

En otras palabras, como ilustra la [esta historia breve](https://cajondeherramientas.com.ar/index.php/2016/05/05/en-bandeja-de-plata-una-historia-sobre-los-privilegios/) de [Toby Morris](https://en.wikipedia.org/wiki/Toby_Morris_(cartoonist)) es que resulta necesario escalar los datos para que todas las características tengan la misma importancia y ninguna esté dominada por otra.
